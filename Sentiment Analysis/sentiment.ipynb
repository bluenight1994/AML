{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import glob\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Preprocessing and c) Split training and testing set\n",
    "Since these sentences are online reviews, they may contain significant amounts of noise and garbage.\n",
    "\n",
    "* Lowercase all of the words\n",
    "* Lemmatization of all the words\n",
    "* Strip punctuation\n",
    "* Strip the stop words\n",
    "\n",
    "for each file, please use the first 400 in- stances for each label as the training set and the remaining 100 instances as testing set. In total, there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 2400 reviews for training\n",
      "600 reviews for testing.\n"
     ]
    }
   ],
   "source": [
    "entries = [] # words for each entity for all files\n",
    "labels = [] # sentiment label for all files\n",
    "\n",
    "# for Spliting training and testing set\n",
    "test_X = []\n",
    "test_Y = []\n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "for filename in glob.glob('./data/*.txt'):\n",
    "    f = open(filename, 'r')\n",
    "    positiveCnt = 0; negativeCnt = 0\n",
    "    sampThreshold = 800\n",
    "    for line in f:\n",
    "        temp = line.split('\\t') # split strings and sentiment label\n",
    "        label = temp[1][0]\n",
    "        labels.append(label) # add sentiment label\n",
    "\n",
    "        # remove all signs and spaces\n",
    "        # 'filter' is for removing all null strings after regluar expression applied\n",
    "        # 're' is for applying regluar exp so that signs would be remove\n",
    "        # 'lower' is for make all words become lower case\n",
    "        sentence = filter(None, re.split(r'[^a-zA-Z\\']', temp[0].lower()))\n",
    "\n",
    "        # Then we will move all stop words such as 'to', 'I', 'that', and 'the'\n",
    "        # And apply the Wordnet Lemmatizer to remove any affix\n",
    "        stop = set(stopwords.words('english'))\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        parsed = [lmtzr.lemmatize(word) for word in sentence if word not in stop]\n",
    "        entries.append(parsed)\n",
    "        \n",
    "        # Split data based on its label and sperate them into different group\n",
    "        # this make sure the training data is well balanced.\n",
    "        if label == '1' and positiveCnt < 400:\n",
    "            train_X.append(parsed)\n",
    "            train_Y.append(label)\n",
    "            positiveCnt += 1\n",
    "            continue\n",
    "        if label == '0' and negativeCnt < 400:\n",
    "            train_X.append(parsed)\n",
    "            train_Y.append(label)\n",
    "            negativeCnt += 1\n",
    "            continue\n",
    "        test_X.append(parsed)\n",
    "        test_Y.append(label)\n",
    "        \n",
    "\n",
    "print 'There are totally ' + str(len(train_X)) + ' reviews for training'\n",
    "print str(len(test_X)) + ' reviews for testing.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Bag of Words model\n",
    "Extract features and then represent each review using bag of words model, i.e.,  \n",
    "every word in the review becomes its own element in a feature vector.In order to do this,  \n",
    "1. make one pass through all the reviews in the training set and build a dictionary of unique words.  \n",
    "2. Then, make another pass through the review in both the training set and testing set and count up the occurrences of each word in your dictionary. \n",
    "\n",
    "The ith element of a reviewâ€™s feature vector is the number of occurrences of the i th dictionary word in the review.  \n",
    "Implement the bag of words model and report feature vectors of any two reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a table with all words\n",
    "    the table is a 1*d table, with each parsed word\n",
    "\n",
    "If directly print the matrix, there will be char u'*', it is because json parser read string\n",
    "    as unicode. It will be gone when fetch directly (e.g. ingredientsTable[0] print will not show the u')\n",
    "\"\"\"\n",
    "def BagOfWordModel(train_X, test_Y):\n",
    "    wordTable = list()  # A table contains all words in training set\n",
    "    for i in range(len(train_X)):\n",
    "        for j in range(len(train_X[i])):\n",
    "            if train_X[i][j] not in wordTable:\n",
    "                wordTable.append(train_X[i][j])\n",
    "    # wordTable = list(wordTable)\n",
    "\n",
    "    train_X_data = np.zeros((len(train_X), len(wordTable)))  # A table with 0 and 1 to indicate\n",
    "    for i in range(len(train_X)):\n",
    "        for j in range(len(train_X[i])):\n",
    "            train_X_data[i][wordTable.index(train_X[i][j])] += 1\n",
    "        \n",
    "    test_X_data = np.zeros((len(test_X), len(wordTable)))  # A table with 0 and 1 to indicate\n",
    "    for i in range(len(test_X)):\n",
    "        for j in range(len(test_X[i])):\n",
    "            if test_X[i][j] in wordTable:  # Only add words that included in the train set\n",
    "                test_X_data[i][wordTable.index(test_X[i][j])] += 1\n",
    "                \n",
    "    print train_X_data[0]\n",
    "    print train_X_data[1]\n",
    "    print train_X_data[0].shape\n",
    "    return train_X_data, test_X_data, wordTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Pick your postprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here i choose to use L1 Norm postprocessing strategy\n",
    "# this part to be discussed. not very sure\n",
    "def postprocessing(train_X_data):\n",
    "    for i in range(len(train_X_data)):\n",
    "        x = sum([k*k for k in train_X_data[i]])\n",
    "        for j in range(len(wordTable)):\n",
    "            train_X_data[i][j] /= math.sqrt(x)\n",
    "    return train_X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def distance(vecA, vecB):\n",
    "    return sqrt(sum(power(vecA-vecB, 2)))\n",
    "\n",
    "\n",
    "# use random value to intialize the centroids\n",
    "def initialCentroids(dataSet, k):\n",
    "    n = dataSet.shape[1]\n",
    "    centroids = mat(zeros((k,n)))\n",
    "    for j in range(n):\n",
    "        minE = min(dataSet[:,j])\n",
    "        maxE = max(dataSet[:,j])\n",
    "        for i in range(k):\n",
    "            centroids[i,j] = minE + float(maxE - minE) * random.rand()\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def kMeans(dataSet, k):\n",
    "    m = dataSet.shape[0]\n",
    "    result = mat(zeros((m,2)))\n",
    "    centroids = initialCentroids(dataSet, k)\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        converged = True\n",
    "        for i in range(m):\n",
    "            minDist, minIndex = np.inf, -1\n",
    "            for j in range(k):\n",
    "                dis = distance(centroids[j,:], dataSet[i,:])\n",
    "                if dis < minDist:\n",
    "                    minDist, minIndex = dis, j\n",
    "            if result[i,0] != minIndex: converged = False\n",
    "            result[i,:] = minIndex, minDist*minDist\n",
    "        for ci in range(k):\n",
    "            points = []\n",
    "            for i in range(m):\n",
    "                if result[i,0] == ci: points.append(dataSet[i])\n",
    "            centroids[ci,:] = mean(array(points), axis = 0)\n",
    "    return centroids, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster Evaluator\n",
    "# compute the ratio of positive & negative previews in each cluster\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate_cluster(train_X_data):\n",
    "    centroids, result = kMeans(train_X_data, 2)\n",
    "    #colors=['#000000','#FFFFFF','#FF0000','#00FF00','#0000FF']\n",
    "    print centroids\n",
    "    #for i in range(2):\n",
    "    #    plt.scatter(result[labels==i,0], result[labels==i,1], c=colors[i])\n",
    "    #    plt.scatter(centroids[i,0], centroids[i,1], c=colors[i], marker='x', s=200, linewidths=2)\n",
    "    #plt.show()\n",
    "    \n",
    "    part1 = []\n",
    "    part2 = []\n",
    "    for a,b,c in zip(train_X_data, result.tolist(),train_Y):\n",
    "        if b[0] == 0: part1.append((a,c))\n",
    "        else: part2.append((a,c))\n",
    "    cnt1, cnt2 = 0, 0\n",
    "    for m,n in part1:\n",
    "        if n == '0': cnt1 += 1\n",
    "        else: cnt2 += 1\n",
    "    cnt3, cnt4 = 0,0\n",
    "    for m,n in part2:\n",
    "        if n == '0': cnt3 += 1\n",
    "        else: cnt4 += 1\n",
    "    print \"cluster1 has \" + str(len(part1)) + \" reviews\"\n",
    "    print \"negative: \" + str(cnt1) +\" \",\n",
    "    print \"posive: \" + str(cnt2)\n",
    "    print \"cluster2 has \" + str(len(part2)) + \" reviews\"\n",
    "    print \"negative: \" + str(cnt3) +\" \",\n",
    "    print \"posive: \" + str(cnt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## h) N-gram Model. N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NgramModel(train_X, test_X):\n",
    "    dictionary = list()\n",
    "    for i in range(len(train_X)):\n",
    "        for j in range(len(train_X[i])-1):\n",
    "            pair = train_X[i][j]+\" \"+train_X[i][j+1]\n",
    "            if pair not in dictionary:\n",
    "                dictionary.append(pair)\n",
    "    #dictionary = list(dictionary)\n",
    "    train_X_data = np.zeros((len(train_X), len(dictionary)))  # A table with 0 and 1 to indicate\n",
    "    for i in range(len(train_X)):\n",
    "        for j in range(len(train_X[i])-1):\n",
    "            pair = train_X[i][j]+\" \"+train_X[i][j+1]\n",
    "            train_X_data[i][dictionary.index(pair)] += 1\n",
    "        \n",
    "    test_X_data = np.zeros((len(test_X), len(dictionary)))  # A table with 0 and 1 to indicate\n",
    "    for i in range(len(test_X)):\n",
    "        for j in range(len(test_X[i])-1):\n",
    "            pair = test_X[i][j]+\" \"+test_X[i][j+1]\n",
    "            if pair in dictionary:  # Only add words that included in the train set\n",
    "                test_X_data[i][dictionary.index(pair)] += 1\n",
    "    return train_X_data, test_X_data, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering on Bag of Word Model\n",
      "\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "(4091,)\n",
      "[[ 0.01791667  0.00541667  0.00833333 ...,  0.00041667  0.00041667\n",
      "   0.00041667]\n",
      " [        nan         nan         nan ...,         nan         nan\n",
      "          nan]]\n",
      "cluster1 has 2400 reviews\n",
      "negative: 1200  posive: 1200\n",
      "cluster2 has 0 reviews\n",
      "negative: 0  posive: 0\n",
      "\n",
      "Logistic Regression with Bag of Word Model\n",
      "accuracy: 0.803333333333\n",
      "[[257  43]\n",
      " [ 75 225]]\n",
      "great\n",
      "delicious\n",
      "love\n",
      "excellent\n",
      "nice\n",
      "loved\n",
      "fantastic\n",
      "awesome\n",
      "amazing\n",
      "liked\n",
      "\n",
      "Clustering on N-gram Model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Zephyr/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        nan         nan         nan ...,         nan         nan\n",
      "          nan]\n",
      " [ 0.00041667  0.00041667  0.00041667 ...,  0.00041667  0.00041667\n",
      "   0.00041667]]\n",
      "cluster1 has 0 reviews\n",
      "negative: 0  posive: 0\n",
      "cluster2 has 2400 reviews\n",
      "negative: 1200  posive: 1200\n",
      "\n",
      "Logic Regression with N-gram Model\n",
      "accuracy: 0.638333333333\n",
      "[[267  33]\n",
      " [184 116]]\n",
      "work great\n",
      "highly recommend\n",
      "one best\n",
      "great phone\n",
      "great product\n",
      "food good\n",
      "really good\n",
      "easy use\n",
      "good price\n",
      "great food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Zephyr/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:36: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print \"Clustering on Bag of Word Model\"\n",
    "print\n",
    "train, test, wordTable= BagOfWordModel(train_X, test_X)\n",
    "# train = postprocessing(train)\n",
    "evaluate_cluster(train)\n",
    "print\n",
    "print \"Logistic Regression with Bag of Word Model\"\n",
    "cls = linear_model.LogisticRegression()\n",
    "print \"accuracy:\",\n",
    "res = cls.fit(train, train_Y)\n",
    "print res.score(test, test_Y)\n",
    "print confusion_matrix(test_Y, res.predict(test))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    print wordTable[np.where(res.coef_[0] == i)[0]]\n",
    "\n",
    "\n",
    "print\n",
    "print \"Clustering on N-gram Model\"\n",
    "print\n",
    "train, test, wordTable = NgramModel(train_X, test_X)\n",
    "# train = postprocessing(train)\n",
    "evaluate_cluster(train)\n",
    "print\n",
    "print \"Logic Regression with N-gram Model\"\n",
    "cls = linear_model.LogisticRegression()\n",
    "print \"accuracy:\",\n",
    "res = cls.fit(train, train_Y)\n",
    "print res.score(test, test_Y)\n",
    "print confusion_matrix(test_Y, res.predict(test))\n",
    "nmax = heapq.nlargest(10, res.coef_[0])\n",
    "for i in nmax:\n",
    "    print wordTable[np.where(res.coef_[0] == i)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) PCA for bag of word model.\n",
    "The feature in *bag of words* has large redundancy.  \n",
    "Implement PCA first to reduce dimension of features to 10, 50, 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "(4091,)\n",
      "accuracy with r = 100: 0.696666666667\n",
      "accuracy with r = 50: 0.681666666667\n",
      "accuracy with r = 10 0.613333333333\n"
     ]
    }
   ],
   "source": [
    "def PCA(train, test, r):\n",
    "    # subtract mean\n",
    "    mean_vector = np.mean(train, axis=0)\n",
    "    X = train - mean_vector\n",
    "    X = np.dot(X.transpose(), X)\n",
    "    U, s, V = np.linalg.svd(X, full_matrices = True)\n",
    "    F = np.dot(train, (V[:r,:]).T)\n",
    "    K = np.dot(test, (V[:r,:]).T)\n",
    "    return F, K\n",
    "# test PCA implement choose r = 10, 50, 100\n",
    "train, test, wordTable = BagOfWordModel(train_X, test_X)\n",
    "train, test = PCA(train, test, 100)\n",
    "cls = linear_model.LogisticRegression()\n",
    "print \"accuracy with r = 100:\",\n",
    "print cls.fit(train, train_Y).score(test, test_Y)\n",
    "train, test = PCA(train, test, 50)\n",
    "cls = linear_model.LogisticRegression()\n",
    "print \"accuracy with r = 50:\",\n",
    "print cls.fit(train, train_Y).score(test, test_Y)\n",
    "train, test = PCA(train, test, 10)\n",
    "cls = linear_model.LogisticRegression()\n",
    "print \"accuracy with r = 10\",\n",
    "print cls.fit(train, train_Y).score(test, test_Y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
